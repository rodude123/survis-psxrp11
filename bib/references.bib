@article{1,
	title = {Study of {Artificial} {Intelligence} into {Checkers} {Game} using {HTML} and {JavaScript}},
	volume = {864},
	issn = {1757-899X},
	url = {https://dx.doi.org/10.1088/1757-899X/864/1/012091},
	doi = {10.1088/1757-899X/864/1/012091},
	abstract = {Artificial Intelligence (AI) in the game industry matures and continues to expand due to its capabilities to explore and exploit in bringing gaming to life. In this paper, we described the game design for the game Checkers, which is developed by using an AI heuristic approach using HTML and JavaScript. A group of people chosen in this study to test the game, which is a validation process to confirm that the game goes as per requirement. The testing activity ends after the final bug fix and the result shows that the game executes properly as expected. The result indicates that the game meets the objective of the system, which is the game, may learn and explore by itself as a heuristic element.},
	language = {en},
	number = {1},
	urldate = {2023-05-15},
	journal = {IOP Conf. Ser.: Mater. Sci. Eng.},
	author = {Idzham, K. K. and Khalishah, M. W. N. and Steven, Y. W. and Aminuddin, M. S. M. F. and Syawani, H. N. and Zain, A. M. and Yusoff, Y.},
	month = {may},
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {012091},
	file = {IOP Full Text PDF:/home/rodude123/Zotero/storage/8C7QFHT4/Idzham et al. - 2020 - Study of Artificial Intelligence into Checkers Gam.pdf:application/pdf},
}

@inproceedings{2,
	title = {Minimax {Checkers} {Playing} {GUI}: {A} {Foundation} for {AI} {Applications}},
	shorttitle = {Minimax {Checkers} {Playing} {GUI}},
	doi = {10.1109/INTERCON.2018.8526375},
	abstract = {This paper presents an application that uses artificial intelligence (AI) to play checkers. The program application has a MATLAB graphical user interface (GUI) and is equipped with a minimax algorithm. The AI has proven to be successful against the average student at the Universidad de Ingeniería y Tecnología (UTEC) in a game of checkers. The importance of this project resides in providing the first steps at research and implementation of artificial intelligence that could be used in applications such as robotics in manufacturing and project planning. The project accomplished the objectives of creating an AI GUI that could play checkers against UTEC students. As was expected, success of the AI depended on the depth of the minimax tree. The AI struggled against UTEC students while using tree depths of three and four levels; however, the AI proved successful against UTEC students at tree depths of five and six. The optimal level chosen for the checkers AI eventually was five levels of depth because for six or more levels, the time of minimax evaluation delayed too much for fluid human game play.},
	booktitle = {2018 {IEEE} {XXV} {International} {Conference} on {Electronics}, {Electrical} {Engineering} and {Computing} ({INTERCON})},
	author = {Escandon, Elmer R. and Campion, Joseph},
	month = {aug},
	year = {2018},
	keywords = {Artificial intelligence, Games, AI Foundations, Graphical User Interface, Graphical user interfaces, Matlab, Minimax algorithm, Robots, Runtime, Tree graphs},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:/home/rodude123/Zotero/storage/JRY3HZSU/8526375.html:text/html;IEEE Xplore Full Text PDF:/home/rodude123/Zotero/storage/VJ65PEZI/Escandon and Campion - 2018 - Minimax Checkers Playing GUI A Foundation for AI .pdf:application/pdf},
}

@phdthesis{3,
	type = {bachelor},
	title = {Robot {Checkers} {Player}},
	url = {http://repository.sgu.ac.id/2216/},
	abstract = {Technological developments in the field of moving goods are increasing rapidly. Production processes in fields such as the automotive, medical world, and even for household needs use the Pick and Place system for the production process. It has been proven, for a long time that research has improved the Pick and Place system for many sectors, especially how this process can be combined with AI. Because of this, there has been a lot of research on how to make robots to play games with their own brains. For now, most of the game development for AI is board games such as chess, Checkers, Tic Tac Toe, and other board games. Therefore, this project is expected to create a robot that can play board games. By implementing the gantry robot system, the robot will be made to play the Checkers game by integrating the robot with the existing AI checker. The concept of this robot is to use Checkers AI and machine integration with a camera where in this project the robot will compete with humans, the camera will detect it from the game board and then with the learning process from AI, the robot can determine where to move the pieces and it is hoped that the motor will move according to the AI output command of the Checkers Robot. After that the robot will return to its original position and wait for its turn.},
	language = {en},
	urldate = {2023-05-15},
	school = {Swiss German University},
	author = {Geofany, Geofany and Manurung, Edward Boris},
	month = {jul},
	year = {2021},
	file = {Full Text PDF:/home/rodude123/Zotero/storage/J2BD8M74/Geofany and Manurung - 2021 - Robot Checkers Player.pdf:application/pdf;Snapshot:/home/rodude123/Zotero/storage/QR2FCBZA/2216.html:text/html},
}

@article{4,
	title = {Introducing {Individual} and {Social} {Learning} {Into} {Evolutionary} {Checkers}},
	volume = {4},
	issn = {1943-0698},
	doi = {10.1109/TCIAIG.2012.2209424},
	abstract = {In recent years, much research attention has been paid to evolving self-learning game players. Fogel's Blondie24 is just one demonstration of a real success in this field and it has inspired many other scientists. In this paper, evolutionary neural networks, evolved via an evolution strategy, are employed to evolve game-playing strategies for the game of Checkers. In addition, we introduce an individual and social learning mechanism into the learning phase of this evolutionary Checkers system. The best player obtained is tested against an implementation of an evolutionary Checkers program, and also against a player, which has been evolved within a round robin tournament. The results are promising and demonstrate that using individual and social learning enhances the learning process of the evolutionary Checkers system and produces a superior player compared to what was previously possible.},
	number = {4},
	journal = {IEEE Transactions on Computational Intelligence and AI in Games},
	author = {Al-Khateeb, Belal and Kendall, Graham},
	month = {dec},
	year = {2012},
	note = {Conference Name: IEEE Transactions on Computational Intelligence and AI in Games},
	keywords = {Games, Humans, Artificial neural networks, Checkers, Computers, Databases, Educational institutions, evolutionary algorithms, individual and social learning, Machine learning, Neural networks},
	pages = {258--269},
	file = {IEEE Xplore Abstract Record:/home/rodude123/Zotero/storage/2B38J6AG/6243194.html:text/html;IEEE Xplore Full Text PDF:/home/rodude123/Zotero/storage/C6JZ5UNL/Al-Khateeb and Kendall - 2012 - Introducing Individual and Social Learning Into Ev.pdf:application/pdf},
}

@article{5,
	title = {Playing {Chinese} {Checkers} with {Reinforcement} {Learning}},
	abstract = {We built an AI for Chinese checkers using reinforcement learning. The value of each board state is determined via minimaxation of a tree of depth k, while the value of each leaf is approximated by weights and features extracted from the board. Weights are tuned via function approximation. The performance of our modiﬁed minimax strategy with tuned weights stands out among all the other strategies.},
	language = {en},
	author = {He, Sijun and Hu, Wenjie and Yin, Hao},
	file = {He et al. - Playing Chinese Checkers with Reinforcement Learni.pdf:/home/rodude123/Zotero/storage/YH528FQQ/He et al. - Playing Chinese Checkers with Reinforcement Learni.pdf:application/pdf},
}

@inproceedings{6,
	title = {{ACE}-{RL}-{Checkers}: {Improving} automatic case elicitation through knowledge obtained by reinforcement learning in player agents},
	shorttitle = {{ACE}-{RL}-{Checkers}},
	doi = {10.1109/CIG.2015.7317926},
	abstract = {This work proposes a new approach that combines Automatic Case Elicitation with Reinforcement Learning applied to Checkers player agents. This type of combination brings forth the following modifications in relation to those agents that use each of these techniques in isolation: improve the random exploration performed by the Automatic Case Elicitation-based agents and introduce adaptability to the Reinforcement Learning-based agents. In line with the above, the authors present herein the ACE-RL-Checkers player agent, a hybrid system that combines the best abilities from the automatic Checkers players CHEBR and LS-VisionDraughts. CHEBR is an Automatic Case Elicitation-based agent with a learning approach that performs random exploration in the search space. These random explorations allow the agent to present an extremely adaptive and non-deterministic behavior. On the other hand, the high frequency at which decisions are made randomly (mainly in those phases in which the content of the case library is still so scarce) compromises the agent in terms of maintaining a good performance. LS-VisionDraughts is a Multi-Layer Perceptron Neural Network player trained through Reinforcement Learning. Besides having been proven efficient in making decisions, such an agent presents an inconvenience in that it is completely predictable, as the same move is always executed when presented with the same board of play. By combining the best abilities from these players, ACE-RL-Checkers uses knowledge provided from LS-VisionDraughts in order to direct random exploration of the automatic case elicitation technique to more promising regions in the search space. Therewith, the ACE-RL-Checkers gains in terms of performance as well as acquires adaptability in its decision-making - choosing moves based on the current game dynamics. Experiments carried out in tournaments involving these agents confirm the performance superiority of ACE-RL-Checkers when pitted against its adversaries.},
	booktitle = {2015 {IEEE} {Conference} on {Computational} {Intelligence} and {Games} ({CIG})},
	author = {Neto, Henrique Castro and Silva Julia, Rita Maria},
	month = {aug},
	year = {2015},
	note = {ISSN: 2325-4289},
	keywords = {Games, Artificial neural networks, Complexity theory, Libraries, Silicon, Space exploration, Training},
	pages = {328--335},
	file = {IEEE Xplore Abstract Record:/home/rodude123/Zotero/storage/LEMKS3CI/7317926.html:text/html;IEEE Xplore Full Text PDF:/home/rodude123/Zotero/storage/IEBZQLXV/Neto and Silva Julia - 2015 - ACE-RL-Checkers Improving automatic case elicitat.pdf:application/pdf},
}

@misc{7,
	title = {Mastering {Chess} and {Shogi} by {Self}-{Play} with a {General} {Reinforcement} {Learning} {Algorithm}},
	url = {http://arxiv.org/abs/1712.01815},
	doi = {10.48550/arXiv.1712.01815},
	abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
	urldate = {2023-05-15},
	publisher = {arXiv},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	month = {dec},
	year = {2017},
	note = {arXiv:1712.01815 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/rodude123/Zotero/storage/WDDTC3G5/Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a Gene.pdf:application/pdf;arXiv.org Snapshot:/home/rodude123/Zotero/storage/RXK4BWQY/1712.html:text/html},
}

@article{8,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature16961},
	doi = {10.1038/nature16961},
	abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approa<F2>ch to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	language = {en},
	number = {7587},
	urldate = {2023-05-15},
	journal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	month = {jan},
	year = {2016},
	note = {Number: 7587
Publisher: Nature Publishing Group},
	keywords = {Computational science, Computer science, Reward},
	pages = {484--489},
	file = {Full Text PDF:/home/rodude123/Zotero/storage/J2J85X68/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf:application/pdf},
}

@inproceedings{9,
	title = {Training {Deep} {Convolutional} {Neural} {Networks} to {Play} {Go}},
	url = {https://proceedings.mlr.press/v37/clark15.html},
	abstract = {Mastering the game of Go has remained a long-standing challenge to the field of AI. Modern computer Go programs rely on processing millions of possible future positions to play well, but intuitively a stronger and more ’humanlike’ way to play the game would be to rely on pattern recognition rather than brute force computation. Following this sentiment, we train deep convolutional neural networks to play Go by training them to predict the moves made by expert Go players. To solve this problem we introduce a number of novel techniques, including a method of tying weights in the network to ’hard code’ symmetries that are expected to exist in the target function, and demonstrate in an ablation study they considerably improve performance. Our final networks are able to achieve move prediction accuracies of 41.1\% and 44.4\% on two different Go datasets, surpassing previous state of the art on this task by significant margins. Additionally, while previous move prediction systems have not yielded strong Go playing programs, we show that the networks trained in this work acquired high levels of skill. Our convolutional neural networks can consistently defeat the well known Go program GNU Go and win some games against state of the art Go playing program Fuego while using a fraction of the play time.},
	language = {en},
	urldate = {2023-05-15},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Clark, Christopher and Storkey, Amos},
	month = {jun},
	year = {2015},
	note = {ISSN: 1938-7228},
	pages = {1766--1774},
	file = {Full Text PDF:/home/rodude123/Zotero/storage/C6XKJZ82/Clark and Storkey - 2015 - Training Deep Convolutional Neural Networks to Pla.pdf:application/pdf},
}

@article{10,
	title = {Convolutional and {Recurrent} {Neural} {Network} for {Gomoku}},
	abstract = {This paper proposes to use neural networks to solve a simplified version of Gomoku. More specifically, convolutional neural network and multi-dimensional recurrent network are trained separately using high-level human games. While the convolutional neural network was able to learn specific features for Gomoku without prior knowledge, the human game data set proved to be insufficient for a robust performance. The multi-dimensional recurrent network also suffered from the small database even though it was able to occasionally occasionally expert level moves.},
	language = {en},
	author = {Zhang, Rongxiao},
	file = {Zhang - Convolutional and Recurrent Neural Network for Gom.pdf:/home/rodude123/Zotero/storage/W2ID5B7G/Zhang - Convolutional and Recurrent Neural Network for Gom.pdf:application/pdf},
}
